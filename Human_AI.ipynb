{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yoyodahary/ReaserchProject/blob/master/Human_AI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "PXKnEnhL09M4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "import re\n",
        "import h5py\n",
        "import math\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from tabulate import tabulate\n",
        "import os\n",
        "import time\n",
        "import psutil\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.over_sampling import ADASYN\n",
        "from multiprocessing import Process\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "import json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Dkch1NUw1ET6"
      },
      "outputs": [],
      "source": [
        "root = \"\"\n",
        "data_path = root + 'data/'\n",
        "tfidf_path = data_path + \"tf-idf/\"\n",
        "vectors_path = data_path + \"vectors/\"\n",
        "classifications_path = data_path + \"classifications/\"\n",
        "results_path = data_path + \"results/\"\n",
        "\n",
        "df = pd.read_csv(data_path+\"AI_Human_cleaned.csv\", usecols = [\"text\",\"prompt_name\",\"source\",\"label\"])\n",
        "\n",
        "#df = df.sample(n=1000, random_state=42)\n",
        "\n",
        "train_df, test_df = train_test_split(df, test_size=0.25, random_state=42)\n",
        "\n",
        "folder_path = data_path\n",
        "if not os.path.exists(folder_path):\n",
        "    os.makedirs(folder_path)\n",
        "\n",
        "train_df.to_csv(data_path+'train_dataset.csv',index=False)\n",
        "train_df.to_hdf(data_path+'train_dataset.h5', key='train_dataset', mode='w')\n",
        "\n",
        "test_df.to_csv(data_path+'test_dataset.csv',index=False)\n",
        "test_df.to_hdf(data_path+'test_dataset.h5', key='test_dataset', mode='w')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9iwEumS-QFTF",
        "outputId": "21323ec8-f676-44bb-bb52-6023fd4c5bda"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\mehke\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "SPECIAL_CHARCATERS_REMOVAL=r\"\\b\\w+\\b\"\n",
        "TOKEN_PATTERN=\"[^ \\n]+\"\n",
        "\n",
        "# Stop Words\n",
        "nltk.download('stopwords')\n",
        "STOP_WORDS = stopwords.words('english')\n",
        "STOP_WORDS += [word.capitalize() for word in STOP_WORDS]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "U76bq9Yq1PlZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Abbreviations\n",
        "ABBREVIATIONS = pd.read_excel(data_path+'abbreviations_eng.xls')\n",
        "ABBREVIATIONS['abbr'] = ABBREVIATIONS['abbr'].astype(str)\n",
        "ABBREVIATIONS_lowercased = ABBREVIATIONS.copy()\n",
        "ABBREVIATIONS_lowercased['abbr'] = ABBREVIATIONS_lowercased['abbr'].str.lower()\n",
        "ABBREVIATIONS_lowercased['long'] = ABBREVIATIONS_lowercased['long'].str.lower()\n",
        "ABBREVIATIONS = pd.concat([ABBREVIATIONS, ABBREVIATIONS_lowercased], ignore_index=True)\n",
        "ABBR_PATTERN = r'\\b(?:' + '|'.join(map(re.escape, ABBREVIATIONS['abbr'])) + r')\\b'\n",
        "def expand_abbreviations(text):\n",
        "    def replace(match):\n",
        "        return ABBREVIATIONS.loc[ABBREVIATIONS['abbr'] == match.group(0), 'long'].iloc[0]\n",
        "\n",
        "    return re.sub(ABBR_PATTERN, replace, text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "onWW4qVl1W_F"
      },
      "outputs": [],
      "source": [
        "def read_texts(train_or_test):\n",
        "  \"\"\"\n",
        "  reads the data from the train tweets sheet\n",
        "  :return: the data as pandas data set\n",
        "  \"\"\"\n",
        "\n",
        "  store = pd.HDFStore(data_path+f'{train_or_test}_dataset.h5')\n",
        "  df = store.select(f'{train_or_test}_dataset')\n",
        "  store.close()\n",
        "\n",
        "  columns_to_include = [\"text\"]\n",
        "  df = df[columns_to_include]\n",
        "  df.dropna(subset=[\"text\"], inplace=True)\n",
        "  return df\n",
        "\n",
        "def read_classifications(train_or_test=\"train\", classification = \"label\"):\n",
        "  \"\"\"\n",
        "  reads the data from the train tweets sheet\n",
        "  :return: the data as pandas data set\n",
        "  \"\"\"\n",
        "\n",
        "  store = pd.HDFStore(data_path+f'{train_or_test}_dataset.h5')\n",
        "  df = store.select(f'{train_or_test}_dataset')\n",
        "  store.close()\n",
        "\n",
        "  columns_to_include = [classification]\n",
        "  df = df[columns_to_include]\n",
        "  df.dropna(subset=[classification], inplace=True)\n",
        "  return df\n",
        "\n",
        "\n",
        "def get_terms():\n",
        "  columns= [\"nt\", \"f\", \"tf\",\"idf\",\"tfidf\"]\n",
        "  meanings= [\"Number of different documents that the word appears in.\",\n",
        "   \"Number of appearances of the word in all documents.\",\n",
        "   \"Term frequency.\",\n",
        "   \"Inverse document frequency.\",\n",
        "   \"Term fruquency multiplied by inverse document frequency.\"]\n",
        "  return columns, meanings\n",
        "\n",
        "\n",
        "def write_tf_idf_chart(preprocessing,path = tfidf_path):\n",
        "  train_text = read_texts(\"train\")\n",
        "  vectorizer = VECTORIZERS[preprocessing]\n",
        "  if \"L\" in preprocessing and \"O\" in preprocessing:\n",
        "      train_text[\"text\"] = \"LowerCode \" + train_text[\"text\"]\n",
        "  elif \"L\" in preprocessing:\n",
        "      train_text[\"text\"] = train_text[\"text\"].str.lower()\n",
        "\n",
        "\n",
        "  sparse_matrix = vectorizer.fit_transform(train_text[\"text\"])\n",
        "  dense_matrix = sparse_matrix.toarray()\n",
        "  sig_f = sparse_matrix.sum()\n",
        "\n",
        "  print(f\"Me: sig_f is:{sig_f}\")\n",
        "\n",
        "  # Get the vocabulary from the vectorizer object\n",
        "  vocab = vectorizer.get_feature_names_out()\n",
        "  df = pd.DataFrame(columns=['word', 'nt', 'f', 'tf', 'idf', 'tf-idf'])\n",
        "\n",
        "\n",
        "  print(f\"calculating the tfidf table for {preprocessing}:\")\n",
        "  for word , index in tqdm(vectorizer.vocabulary_.items()):\n",
        "      # print(word)\n",
        "      # The column of tf-idf values of the specific word, as the rows are the tweets\n",
        "      word_column=dense_matrix[:, index]\n",
        "      # Count the number of texts the word appears in\n",
        "      nt = np.count_nonzero(word_column)\n",
        "      # Count the number of times the word is used in all tweets\n",
        "      f = np.sum(word_column)\n",
        "      # term frequency in a document compared to the number of terms in the corpus\n",
        "      tf = f / sig_f\n",
        "      # shape[0] is the number of rows, aka the number of texts\n",
        "      n = dense_matrix.shape[0]\n",
        "      # Calculate the idf value\n",
        "      idf =np. log(n / (nt + 1))\n",
        "      # Calculate the tf-idf value\n",
        "      tf_idf = tf * idf\n",
        "      # append all the data we collected so far to each word to the data frame\n",
        "      new_df = pd.DataFrame({'word': word, 'nt': nt, 'f': f, 'tf': tf, 'idf': idf, 'tf-idf': tf_idf}, index=[0])\n",
        "\n",
        "      df = pd.concat([df, new_df], ignore_index=True)\n",
        "  print(\"done!\")\n",
        "  df = df.sort_values('tf-idf', ascending=False)\n",
        "\n",
        "  df.to_csv(path+f\"tfidf_table_{preprocessing}.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Gv_S6ms41k3n"
      },
      "outputs": [],
      "source": [
        "NT = 1  # the minimum nuber of tweets a word in tf idf should show\n",
        "NUMBER_OF_WORDS = [1000,2000,3000,4000,5000]  # the number of the highest tfidf words\n",
        "VECTORIZERS={\n",
        "    # the tf-idf vectorizers by preprocessing method\n",
        "    #  'N' : CountVectorizer(min_df=NT,lowercase=False, token_pattern=TOKEN_PATTERN),#None, baseline\n",
        "\n",
        "    #  'S':CountVectorizer(min_df=NT,lowercase=False, token_pattern=TOKEN_PATTERN,stop_words=STOP_WORDS),#Stop word removal\n",
        "    #  'C':CountVectorizer(min_df=NT,lowercase=False, token_pattern=SPECIAL_CHARCATERS_REMOVAL),#special Characters removal\n",
        "    #  'O':CountVectorizer(min_df=NT,lowercase=False, token_pattern=TOKEN_PATTERN, preprocessor=expand_abbreviations),#Open abbreviations\n",
        "    # # '3gram' : CountVectorizer(min_df=NT,lowercase=False, analyzer='char', ngram_range=(3, 3)),\n",
        "    # # '4gram' : CountVectorizer(min_df=NT,lowercase=False, analyzer='char', ngram_range=(4, 4)),\n",
        "     'L' : CountVectorizer(min_df=NT,lowercase=True, token_pattern=TOKEN_PATTERN),# Lowercase\n",
        "\n",
        "    # # pairing preprocssing methods\n",
        "    'SC':CountVectorizer(min_df=NT,lowercase=False, token_pattern=SPECIAL_CHARCATERS_REMOVAL,stop_words=STOP_WORDS),\n",
        "    # 'SO':CountVectorizer(min_df=NT,lowercase=False, token_pattern=TOKEN_PATTERN,stop_words=STOP_WORDS,preprocessor=expand_abbreviations),\n",
        "    'SL':CountVectorizer(min_df=NT,lowercase=True, token_pattern=TOKEN_PATTERN,stop_words=STOP_WORDS),\n",
        "    # 'CO':CountVectorizer(min_df=NT,lowercase=False, token_pattern=SPECIAL_CHARCATERS_REMOVAL,preprocessor=expand_abbreviations),\n",
        "    'CL':CountVectorizer(min_df=NT,lowercase=True, token_pattern=SPECIAL_CHARCATERS_REMOVAL),\n",
        "    # 'OL':CountVectorizer(min_df=NT,lowercase=True, token_pattern=TOKEN_PATTERN,preprocessor=expand_abbreviations),\n",
        "\n",
        "    # # Trio preprocessing methods\n",
        "    # 'SCO':CountVectorizer(min_df=NT,lowercase=False, token_pattern=SPECIAL_CHARCATERS_REMOVAL,stop_words=STOP_WORDS,preprocessor=expand_abbreviations),\n",
        "    # 'COL':CountVectorizer(min_df=NT,lowercase=True, token_pattern=SPECIAL_CHARCATERS_REMOVAL,preprocessor=expand_abbreviations),\n",
        "    # 'SOL':CountVectorizer(min_df=NT,lowercase=True, token_pattern=TOKEN_PATTERN,stop_words=STOP_WORDS, preprocessor=expand_abbreviations),\n",
        "    'SCL':CountVectorizer(min_df=NT,lowercase=True, token_pattern=SPECIAL_CHARCATERS_REMOVAL,stop_words=STOP_WORDS),\n",
        "\n",
        "    # # # All preprocessing\n",
        "    # 'SCOL':CountVectorizer(min_df=NT,lowercase=True, token_pattern=SPECIAL_CHARCATERS_REMOVAL,stop_words=STOP_WORDS,preprocessor=expand_abbreviations)\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "P_M7v3Wc1nqq",
        "outputId": "5db04750-ecb2-4709-a145-6248736e1528"
      },
      "outputs": [],
      "source": [
        "\n",
        "#itirate over each preprocessing method and writes the tfidf chart\n",
        "for preprocessing, vectorizer in VECTORIZERS.items():\n",
        "  print(\"------------------------------------------\")\n",
        "  print(f\"writing the {preprocessing} tf-idf chart:\")\n",
        "  write_tf_idf_chart(preprocessing)\n",
        "  print(\"------------------------------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "EpTqOuwZ15se"
      },
      "outputs": [],
      "source": [
        "CLASSIFY = ['label']\n",
        "\n",
        "def write_classification(text, classification, set):\n",
        "    classified_text_df = pd.DataFrame(columns=[\"class: \"+classification])\n",
        "    if classification == 'label':\n",
        "        classified_text_df[\"class: \"+classification] = text[\"label\"].apply(lambda x: 0 if x == 0 else 1)\n",
        "    folder_path = 'classification/'\n",
        "    if not os.path.exists(folder_path):\n",
        "        os.makedirs(folder_path)\n",
        "    classified_text_df.to_csv(classifications_path+f'{set}_classification_{classification}.csv', index=False)    \n",
        "    classified_text_df.to_hdf(classifications_path+f'{set}_classification_{classification}.h5', key='classification', mode='w')\n",
        "\n",
        "def write_vectors(tfidf_vocabulary,vectors,set,amount,preprocessing):\n",
        "  dense_vectors = vectors.toarray()\n",
        "  df = pd.DataFrame(dense_vectors, columns=tfidf_vocabulary)\n",
        "  folder_path = 'vectors/'\n",
        "  if not os.path.exists(folder_path):\n",
        "        os.makedirs(folder_path)\n",
        "  df.to_csv(f'vectors/{preprocessing}_{set}_vectors_{amount}.csv',index=False)\n",
        "  df.to_hdf(f'vectors/{preprocessing}_{set}_vectors_{amount}.h5', key='vectors', mode='w')\n",
        "\n",
        "def get_words(amount,preprocessing):\n",
        "  words = pd.read_csv(tfidf_path+f'tfidf_table_{preprocessing}.csv')\n",
        "  words = words.head(amount)\n",
        "  words = words['word'].tolist()\n",
        "  return words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_texts = read_texts(\"train\")\n",
        "test_texts = read_texts(\"test\")\n",
        "for amount in NUMBER_OF_WORDS:\n",
        "  print(amount)\n",
        "  VECTORIZERS={ # the tf-idf vectorizers by preprocessing method\n",
        "    'N' : TfidfVectorizer(vocabulary=get_words(amount,'N'),min_df=NT,lowercase=False, token_pattern=TOKEN_PATTERN),#None, baseline\n",
        "    \n",
        "    'S':TfidfVectorizer(vocabulary=get_words(amount,'S'),min_df=NT,lowercase=False, token_pattern=TOKEN_PATTERN,stop_words=STOP_WORDS),#Stop word removal\n",
        "    'C':TfidfVectorizer(vocabulary=get_words(amount,'C'),min_df=NT,lowercase=False, token_pattern=SPECIAL_CHARCATERS_REMOVAL),#special Characters removal\n",
        "    # 'O':TfidfVectorizer(vocabulary=get_words(amount,'O'),min_df=NT,lowercase=False, token_pattern=TOKEN_PATTERN, preprocessor=expand_abbreviations),#Open abbreviations\n",
        "    'L' : TfidfVectorizer(vocabulary=get_words(amount,'L'),min_df=NT,lowercase=True, token_pattern=TOKEN_PATTERN),# Lowercase\n",
        "    \n",
        "    # pairing preprocssing methods\n",
        "    'SC':TfidfVectorizer(vocabulary=get_words(amount,'SC'),min_df=NT,lowercase=False, token_pattern=SPECIAL_CHARCATERS_REMOVAL,stop_words=STOP_WORDS),\n",
        "    # 'SO':TfidfVectorizer(vocabulary=get_words(amount,'SO'),min_df=NT,lowercase=False, token_pattern=TOKEN_PATTERN,stop_words=STOP_WORDS,preprocessor=expand_abbreviations),\n",
        "    'SL':TfidfVectorizer(vocabulary=get_words(amount,'SL'),min_df=NT,lowercase=True, token_pattern=TOKEN_PATTERN,stop_words=STOP_WORDS),\n",
        "    # 'CO':TfidfVectorizer(vocabulary=get_words(amount,'CO'),min_df=NT,lowercase=False, token_pattern=SPECIAL_CHARCATERS_REMOVAL,preprocessor=expand_abbreviations),\n",
        "    'CL':TfidfVectorizer(vocabulary=get_words(amount,'CL'),min_df=NT,lowercase=True, token_pattern=SPECIAL_CHARCATERS_REMOVAL),\n",
        "    # 'OL':TfidfVectorizer(vocabulary=get_words(amount,'OL'),min_df=NT,lowercase=True, token_pattern=TOKEN_PATTERN,preprocessor=expand_abbreviations),\n",
        "    \n",
        "    # Trio preprocessing methods\n",
        "    # 'SCO':TfidfVectorizer(vocabulary=get_words(amount,'SCO'),min_df=NT,lowercase=False, token_pattern=SPECIAL_CHARCATERS_REMOVAL,stop_words=STOP_WORDS,preprocessor=expand_abbreviations),\n",
        "    # 'COL':TfidfVectorizer(vocabulary=get_words(amount,'COL'),min_df=NT,lowercase=True, token_pattern=SPECIAL_CHARCATERS_REMOVAL,preprocessor=expand_abbreviations),\n",
        "    # 'SOL':TfidfVectorizer(vocabulary=get_words(amount,'SOL'),min_df=NT,lowercase=True, token_pattern=TOKEN_PATTERN,stop_words=STOP_WORDS, preprocessor=expand_abbreviations),\n",
        "    'SCL':TfidfVectorizer(vocabulary=get_words(amount,'SCL'),min_df=NT,lowercase=True, token_pattern=SPECIAL_CHARCATERS_REMOVAL,stop_words=STOP_WORDS),\n",
        "    \n",
        "    # All preprocessing\n",
        "    # 'SCOL':TfidfVectorizer(vocabulary=get_words(amount,'SCOL'),min_df=NT,lowercase=True, token_pattern=SPECIAL_CHARCATERS_REMOVAL,stop_words=STOP_WORDS,preprocessor=expand_abbreviations)\n",
        "}\n",
        "  for preprocessing, vectorizer in tqdm(VECTORIZERS.items()):\n",
        "    tfidf_vocabulary=get_words(amount,preprocessing)\n",
        "    train_vectors = vectorizer.fit_transform(train_texts[\"text\"])\n",
        "    write_vectors(tfidf_vocabulary,train_vectors,\"train\",amount,preprocessing)\n",
        "\n",
        "    test_vectors = vectorizer.fit_transform(test_texts[\"text\"])\n",
        "    write_vectors(tfidf_vocabulary,test_vectors,\"test\",amount,preprocessing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "LABELS = df['label'].to_numpy()\n",
        "\n",
        "OVERSAMPLERS = {\n",
        "    'ROS': RandomOverSampler(random_state=42),\n",
        "    'SMOTE': SMOTE(random_state=42),\n",
        "    'ADASYN': ADASYN(random_state=42),\n",
        "    'NONE': None\n",
        "}\n",
        "\n",
        "ML_CLASSIFIERS = {\n",
        "    'RandomForest': RandomForestClassifier(random_state=42),\n",
        "    'SVC': SVC(random_state=42),\n",
        "    'MLP': MLPClassifier(random_state=42),\n",
        "    'MultinomialNB': MultinomialNB(),\n",
        "    'LogisticRegression': LogisticRegression(random_state=42)\n",
        "}\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_vectors(set,amount,preprocessing):\n",
        "  vectors = pd.read_hdf(vectors_path+f'{preprocessing}_{set}_vectors_{amount}.h5')\n",
        "  return vectors\n",
        "\n",
        "\n",
        "\n",
        "def oversample(oversampler, amount, preprocessing):\n",
        "    train_vectors = load_vectors(\"train\",amount,preprocessing)\n",
        "    train_labels = read_classifications(\"train\")\n",
        "\n",
        "    oversampler = OVERSAMPLERS[oversampler]\n",
        "\n",
        "    if oversampler is not None:\n",
        "        train_vectors, train_labels = oversampler.fit_resample(train_vectors, train_labels)\n",
        "\n",
        "    return train_vectors, train_labels\n",
        "\n",
        "def classify(x_train, y_train, x_test, y_test, classifier):\n",
        "    classifier = ML_CLASSIFIERS[classifier]\n",
        "    classifier.fit(x_train, y_train)\n",
        "    y_pred = classifier.predict(x_test)\n",
        "    return y_pred\n",
        "\n",
        "def evaluate(y_test, y_pred):\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    roc_auc = roc_auc_score(y_test, y_pred)\n",
        "    results = {\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'accuracy': accuracy,\n",
        "        'recall': recall,\n",
        "        'roc_auc': roc_auc\n",
        "    }\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "amount: 1000, preprocessing: N, oversampler: ROS, classifier: RandomForest\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\mehke\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\base.py:1473: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  return fit_method(estimator, *args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "amount: 1000, preprocessing: N, oversampler: ROS, classifier: SVC\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\mehke\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\utils\\validation.py:1310: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "amount: 1000, preprocessing: N, oversampler: ROS, classifier: MLP\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\mehke\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1105: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "amount: 1000, preprocessing: N, oversampler: ROS, classifier: MultinomialNB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\mehke\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\utils\\validation.py:1310: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "amount: 1000, preprocessing: N, oversampler: ROS, classifier: LogisticRegression\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\mehke\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\utils\\validation.py:1310: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "amount: 1000, preprocessing: N, oversampler: SMOTE, classifier: RandomForest\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\mehke\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\base.py:1473: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  return fit_method(estimator, *args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "amount: 1000, preprocessing: N, oversampler: SMOTE, classifier: SVC\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\mehke\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\utils\\validation.py:1310: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        }
      ],
      "source": [
        "for amount in NUMBER_OF_WORDS:\n",
        "  for preprocessing in VECTORIZERS.keys():\n",
        "    for oversampler in OVERSAMPLERS.keys():\n",
        "      for classifier in ML_CLASSIFIERS.keys():\n",
        "        print(f\"amount: {amount}, preprocessing: {preprocessing}, oversampler: {oversampler}, classifier: {classifier}\")\n",
        "        train_vectors, train_labels = oversample(oversampler, amount, preprocessing)\n",
        "        test_vectors = load_vectors(\"test\",amount,preprocessing)\n",
        "        test_labels = read_classifications(\"test\")\n",
        "\n",
        "        y_pred = classify(train_vectors, train_labels, test_vectors, test_labels, classifier)\n",
        "        results = evaluate(test_labels, y_pred)\n",
        "\n",
        "        file_name = f'{classifier}.json'\n",
        "\n",
        "        path_to_save = results_path+f'{preprocessing}/{amount}/{oversampler}/'+file_name\n",
        "\n",
        "        # Save the results\n",
        "        with open(path_to_save, 'w') as f:\n",
        "            json.dump(results, f)\n",
        "            \n",
        "\n",
        "\n",
        "\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not os.path.exists(results_path):\n",
        "    os.makedirs(results_path)\n",
        "for preprocessing in VECTORIZERS.keys():\n",
        "  folder = results_path + preprocessing\n",
        "  if not os.path.exists(folder):\n",
        "    os.makedirs(folder)\n",
        "  for amount in NUMBER_OF_WORDS:\n",
        "    amount_folder = folder + '/' + str(amount)\n",
        "    if not os.path.exists(amount_folder):\n",
        "      os.makedirs(amount_folder)\n",
        "      for oversampler in OVERSAMPLERS:\n",
        "        oversampler_folder = amount_folder + '/' + oversampler\n",
        "        if not os.path.exists(oversampler_folder):\n",
        "          os.makedirs(oversampler_folder)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOere4mPPYtV9Bk+zL1/IC4",
      "include_colab_link": true,
      "mount_file_id": "1DGfDuUDU6wW8aTLgco5ex3zYSCEh39ZL",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
